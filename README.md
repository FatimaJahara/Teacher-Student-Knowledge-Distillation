# Teacher-Student-Knowledge-Distillation
MNIST handwritten digit classification and sentiment analysis using knowledge distillation. Knowledge distillation is a technique where a smaller "student" model learns from a larger, more complex "teacher" model.

<img width="468" height="288" alt="image" src="https://github.com/user-attachments/assets/e3e02d13-d4cd-414c-a35d-635ea74b9aa0" />
