{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatimaJahara/Teacher-Student-Knowledge-Distillation/blob/main/vision_distillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowledge Distillation using MNIST (CNN Teacher, Linear Student)\n",
        "\n",
        "\n",
        "**MNIST Handwritten Digit Classification**\n",
        "\n",
        "*   **Teacher Model:** A Convolutional Neural Network (CNN) with two convolutional layers, dropout,\n",
        "    and two fully connected layers.\n",
        "*   **Student Model:** A simpler, single-layer fully connected network with dropout.\n",
        "*   **Distillation:** The student is trained using a combination of the standard cross-entropy loss\n",
        "    (hard targets) and a distillation loss based on the Kullback-Leibler (KL) divergence between the\n",
        "    student's softened output probabilities and the teacher's softened output probabilities.  The\n",
        "    \"softening\" is achieved using a temperature parameter.\n",
        "* **Baseline Comparison:** The student is also trained *without* distillation to provide a\n",
        "    baseline for comparison.\n",
        "* **Dataset**: MNIST\n",
        "\n",
        "\n",
        "**Key Concepts Illustrated:**\n",
        "\n",
        "*   **Knowledge Distillation:**  The core concept of transferring knowledge from a teacher to a\n",
        "    student network.\n",
        "*   **Soft Targets:**  Using the teacher's output probabilities (softened by a temperature\n",
        "    parameter) as targets for the student.\n",
        "*   **Temperature Scaling:**  The use of a temperature parameter to control the \"softness\" of the\n",
        "    probability distributions.\n",
        "*   **KL Divergence:**  Measuring the difference between the teacher's and student's softened\n",
        "    probability distributions.\n",
        "*   **Combined Loss:**  Combining the standard cross-entropy loss with the distillation loss.\n",
        "*   **Model Compression:**  The student model is significantly smaller than the teacher model,\n",
        "    demonstrating potential for model compression.\n",
        "*   **PyTorch:**  Implementation using the PyTorch deep learning framework.\n",
        "*   **Hugging Face Transformers:** Using the transformers library for BERT models.\n",
        "* **DataLoader:** Using PyTorch's DataLoader to efficiently load and batch data.\n",
        "\n",
        "**Structure of the Notebook:**\n",
        "\n",
        "The code is organized into the following sections for both examples:\n",
        "\n",
        "1.  **Model Definition(s):** Defines the `TeacherNet` and `StudentNet` classes (MNIST) and functions\n",
        "    to load/create the teacher and student BERT models (Sentiment Analysis).\n",
        "2.  **Data Loading:**  Includes functions to load and pre-process the MNIST dataset and a class and\n",
        "    function to prepare the sentiment analysis dataset and DataLoaders.\n",
        "3.  **Training Function(s):**  Contains `train_teacher`, `train_student_with_distillation`, `test`,\n",
        " and `train_student`.\n",
        "4.  **Main Execution:**  The `main` function orchestrates the entire process: loading data,\n",
        "    training the teacher, training the student with and without distillation, and reporting results.\n",
        "\n",
        "**How to Run:**\n",
        "\n",
        "1.  **Install Dependencies:**\n",
        "    ```bash\n",
        "    pip install torch torchvision transformers tqdm\n",
        "    ```\n",
        "2.  **Run the Notebook:** Execute the code cells sequentially.  The MNIST dataset will be\n",
        "    downloaded automatically.  The BERT model will also be downloaded.\n",
        "\n",
        "**Expected Output:**\n",
        "\n",
        "The notebook will print training progress (loss values) and final accuracy scores for the\n",
        "teacher model, the student model trained with distillation, and the student model trained without\n",
        "distillation.  You should observe that the distilled student performs better than the baseline\n",
        "student, demonstrating the effectiveness of knowledge distillation. For the BERT example, training\n",
        "and validation metrics will be printed."
      ],
      "metadata": {
        "id": "HLFc06GU2leG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "knpLWJGM0-zT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Define Teacher Model ---\n",
        "\n",
        "\n",
        "class TeacherNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TeacherNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-kWS5K2eF_3q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Define Student Model ---\n",
        "\n",
        "\n",
        "class StudentNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StudentNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)  # Simpler, single layer\n",
        "        self.dropout = nn.Dropout(0.2) # Added dropout\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1) # Flatten the input\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x) # Apply dropout\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "DuDyhha2GHn_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Data Loading (MNIST) ---\n",
        "\n",
        "def get_data_loaders(batch_size=64):\n",
        "    train_loader = DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])),\n",
        "        batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "DWMDbtBiGC6b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Training Teacher Model ---\n",
        "\n",
        "def train_teacher(model, train_loader, optimizer, epochs=3, device='cpu'): # Reduced epochs for Colab\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 100 == 0: # More frequent updates for quicker feedback\n",
        "                print(f'Teacher Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                      f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "        print(f\"Teacher Epoch {epoch} Average Loss: {running_loss / len(train_loader):.6f}\")"
      ],
      "metadata": {
        "id": "HuPx0zsaGeTq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Training Student Model ---\n",
        "\n",
        "def train_student_with_distillation(teacher_model, student_model, train_loader, optimizer, epochs=3, temperature=5.0, alpha=0.5, device='cpu'):  # Reduced epochs\n",
        "    teacher_model.to(device)\n",
        "    student_model.to(device)\n",
        "    teacher_model.eval()  # Teacher set to eval mode\n",
        "    student_model.train()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        running_loss = 0.0\n",
        "        running_distillation_loss = 0.0\n",
        "        running_student_loss = 0.0\n",
        "\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get teacher's logits (soft targets) by freezing the teacher\n",
        "            # No gradients for the teacher; it’s fixed. Also set teacher_model.eval() outside the loop to disable dropout/BN updates.\n",
        "            with torch.no_grad():  # No gradients needed for teacher\n",
        "                teacher_logits = teacher_model(data)\n",
        "\n",
        "            # Student's logits (Student forward)\n",
        "            student_logits = student_model(data)\n",
        "\n",
        "            # --- Distillation Loss ---\n",
        "            \"\"\" Temperature T (>1) softens the distributions, revealing “dark knowledge” (class similarities).\n",
        "            KLDivLoss expects log-probs as input and probs as target—your use is correct.\n",
        "            Multiply by T²: without it, gradients shrink when you divide logits by T. This rescales them back (Hinton et al., 2015). \"\"\"\n",
        "            soft_targets = F.softmax(teacher_logits / temperature, dim=1) # probsfrom teacher\n",
        "            soft_predictions = F.log_softmax(student_logits / temperature, dim=1) # log-probs from student\n",
        "            distillation_loss = F.kl_div(soft_predictions, soft_targets, reduction='batchmean') * (temperature ** 2)\n",
        "\n",
        "            # --- Student Loss (Cross-Entropy) --- Hard label loss\n",
        "            student_loss = F.cross_entropy(student_logits, target)\n",
        "\n",
        "            # --- Combined Loss ---\n",
        "            loss = alpha * distillation_loss + (1 - alpha) * student_loss\n",
        "\n",
        "            # Backprop + logging\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            running_distillation_loss += distillation_loss.item()\n",
        "            running_student_loss += student_loss.item()\n",
        "\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Student Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                      f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f} '\n",
        "                      f'Distill Loss: {distillation_loss.item():.6f}, Student Loss: {student_loss.item():.6f}')\n",
        "\n",
        "        print(f\"Student Epoch {epoch} Average Loss: {running_loss / len(train_loader):.6f}, \"\n",
        "              f\"Average Distillation Loss: {running_distillation_loss / len(train_loader):.6f}, \"\n",
        "              f\"Average Student Loss: {running_student_loss / len(train_loader):.6f}\")"
      ],
      "metadata": {
        "id": "Mxam5gBVGnzS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, device='cpu'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "Ef8J6yUDMXZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Main Execution ---\n",
        "\n",
        "def main():\n",
        "    # Check CUDA availability\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    train_loader, test_loader = get_data_loaders(batch_size=64)\n",
        "\n",
        "    # --- Train Teacher ---\n",
        "    teacher_model = TeacherNet()\n",
        "    teacher_optimizer = optim.Adam(teacher_model.parameters(), lr=0.001) # Use Adam optimizer\n",
        "    start_time = time.time()\n",
        "    train_teacher(teacher_model, train_loader, teacher_optimizer, epochs=3, device=device)\n",
        "    end_time = time.time()\n",
        "    print(f\"Teacher training time: {end_time - start_time:.2f} seconds\")\n",
        "    teacher_accuracy = test(teacher_model, test_loader, device=device)\n",
        "\n",
        "\n",
        "    # --- Train Student with Distillation ---\n",
        "    student_model = StudentNet()\n",
        "    student_optimizer = optim.Adam(student_model.parameters(), lr=0.001) # Use Adam\n",
        "    start_time = time.time()\n",
        "    train_student_with_distillation(teacher_model, student_model, train_loader, student_optimizer, epochs=3, temperature=5.0, alpha=0.7, device=device) #Experiment with T and alpha\n",
        "    end_time = time.time()\n",
        "    print(f\"Student training time (with distillation): {end_time - start_time:.2f} seconds\")\n",
        "    distilled_student_accuracy = test(student_model, test_loader, device=device)\n",
        "\n",
        "\n",
        "    # --- Train Student *without* Distillation (Baseline) ---\n",
        "    student_model_no_distill = StudentNet()\n",
        "    student_optimizer_no_distill = optim.Adam(student_model_no_distill.parameters(), lr=0.001)\n",
        "    start_time = time.time()\n",
        "    train_teacher(student_model_no_distill, train_loader, student_optimizer_no_distill, epochs=3, device=device)  # Reuse train_teacher\n",
        "    end_time = time.time()\n",
        "    print(f\"Student training time (no distillation): {end_time - start_time:.2f} seconds\")\n",
        "    baseline_student_accuracy = test(student_model_no_distill, test_loader, device=device)\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "    print(\"Results:\")\n",
        "    print(f\"Teacher Accuracy: {teacher_accuracy:.2f}%\")\n",
        "    print(f\"Distilled Student Accuracy: {distilled_student_accuracy:.2f}%\")\n",
        "    print(f\"Baseline Student Accuracy (no distillation): {baseline_student_accuracy:.2f}%\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWxHRHSTISyd",
        "outputId": "40271158-6241-45fd-f317-c0fd5cb7d1ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Teacher Epoch: 1 [0/60000 (0%)]\tLoss: 2.298491\n",
            "Teacher Epoch: 1 [6400/60000 (11%)]\tLoss: 0.308479\n",
            "Teacher Epoch: 1 [12800/60000 (21%)]\tLoss: 0.231870\n",
            "Teacher Epoch: 1 [19200/60000 (32%)]\tLoss: 0.071209\n",
            "Teacher Epoch: 1 [25600/60000 (43%)]\tLoss: 0.077788\n",
            "Teacher Epoch: 1 [32000/60000 (53%)]\tLoss: 0.147454\n",
            "Teacher Epoch: 1 [38400/60000 (64%)]\tLoss: 0.186403\n",
            "Teacher Epoch: 1 [44800/60000 (75%)]\tLoss: 0.163878\n",
            "Teacher Epoch: 1 [51200/60000 (85%)]\tLoss: 0.059135\n",
            "Teacher Epoch: 1 [57600/60000 (96%)]\tLoss: 0.098302\n",
            "Teacher Epoch 1 Average Loss: 0.190221\n",
            "Teacher Epoch: 2 [0/60000 (0%)]\tLoss: 0.167610\n",
            "Teacher Epoch: 2 [6400/60000 (11%)]\tLoss: 0.047824\n",
            "Teacher Epoch: 2 [12800/60000 (21%)]\tLoss: 0.017066\n",
            "Teacher Epoch: 2 [19200/60000 (32%)]\tLoss: 0.085853\n",
            "Teacher Epoch: 2 [25600/60000 (43%)]\tLoss: 0.029826\n",
            "Teacher Epoch: 2 [32000/60000 (53%)]\tLoss: 0.071339\n",
            "Teacher Epoch: 2 [38400/60000 (64%)]\tLoss: 0.015113\n",
            "Teacher Epoch: 2 [44800/60000 (75%)]\tLoss: 0.080279\n",
            "Teacher Epoch: 2 [51200/60000 (85%)]\tLoss: 0.097742\n",
            "Teacher Epoch: 2 [57600/60000 (96%)]\tLoss: 0.023022\n",
            "Teacher Epoch 2 Average Loss: 0.079064\n",
            "Teacher Epoch: 3 [0/60000 (0%)]\tLoss: 0.082700\n",
            "Teacher Epoch: 3 [6400/60000 (11%)]\tLoss: 0.006402\n",
            "Teacher Epoch: 3 [12800/60000 (21%)]\tLoss: 0.135173\n",
            "Teacher Epoch: 3 [19200/60000 (32%)]\tLoss: 0.128796\n",
            "Teacher Epoch: 3 [25600/60000 (43%)]\tLoss: 0.042237\n",
            "Teacher Epoch: 3 [32000/60000 (53%)]\tLoss: 0.023078\n",
            "Teacher Epoch: 3 [38400/60000 (64%)]\tLoss: 0.005578\n",
            "Teacher Epoch: 3 [44800/60000 (75%)]\tLoss: 0.048913\n",
            "Teacher Epoch: 3 [51200/60000 (85%)]\tLoss: 0.168914\n",
            "Teacher Epoch: 3 [57600/60000 (96%)]\tLoss: 0.036582\n",
            "Teacher Epoch 3 Average Loss: 0.058591\n",
            "Teacher training time: 577.97 seconds\n",
            "\n",
            "Test set: Accuracy: 9907/10000 (99.07%)\n",
            "\n",
            "Student Epoch: 1 [0/60000 (0%)]\tLoss: 21.548325 Distill Loss: 29.798647, Student Loss: 2.297574\n",
            "Student Epoch: 1 [6400/60000 (11%)]\tLoss: 4.501079 Distill Loss: 6.159436, Student Loss: 0.631579\n",
            "Student Epoch: 1 [12800/60000 (21%)]\tLoss: 3.798120 Distill Loss: 5.076934, Student Loss: 0.814219\n",
            "Student Epoch: 1 [19200/60000 (32%)]\tLoss: 2.641540 Distill Loss: 3.646833, Student Loss: 0.295854\n",
            "Student Epoch: 1 [25600/60000 (43%)]\tLoss: 2.372254 Distill Loss: 3.244631, Student Loss: 0.336707\n",
            "Student Epoch: 1 [32000/60000 (53%)]\tLoss: 2.442231 Distill Loss: 3.219429, Student Loss: 0.628771\n",
            "Student Epoch: 1 [38400/60000 (64%)]\tLoss: 1.821276 Distill Loss: 2.548224, Student Loss: 0.125064\n",
            "Student Epoch: 1 [44800/60000 (75%)]\tLoss: 1.950650 Distill Loss: 2.715207, Student Loss: 0.166683\n",
            "Student Epoch: 1 [51200/60000 (85%)]\tLoss: 2.001614 Distill Loss: 2.760287, Student Loss: 0.231376\n",
            "Student Epoch: 1 [57600/60000 (96%)]\tLoss: 1.889723 Distill Loss: 2.572583, Student Loss: 0.296383\n",
            "Student Epoch 1 Average Loss: 3.062320, Average Distillation Loss: 4.203429, Average Student Loss: 0.399733\n",
            "Student Epoch: 2 [0/60000 (0%)]\tLoss: 2.255976 Distill Loss: 3.030537, Student Loss: 0.448665\n",
            "Student Epoch: 2 [6400/60000 (11%)]\tLoss: 1.550925 Distill Loss: 2.076253, Student Loss: 0.325158\n",
            "Student Epoch: 2 [12800/60000 (21%)]\tLoss: 1.628371 Distill Loss: 2.265908, Student Loss: 0.140785\n",
            "Student Epoch: 2 [19200/60000 (32%)]\tLoss: 1.741909 Distill Loss: 2.344173, Student Loss: 0.336628\n",
            "Student Epoch: 2 [25600/60000 (43%)]\tLoss: 1.408848 Distill Loss: 1.975029, Student Loss: 0.087758\n",
            "Student Epoch: 2 [32000/60000 (53%)]\tLoss: 1.361667 Distill Loss: 1.886738, Student Loss: 0.136502\n",
            "Student Epoch: 2 [38400/60000 (64%)]\tLoss: 1.473303 Distill Loss: 2.036752, Student Loss: 0.158588\n",
            "Student Epoch: 2 [44800/60000 (75%)]\tLoss: 1.448424 Distill Loss: 1.970121, Student Loss: 0.231129\n",
            "Student Epoch: 2 [51200/60000 (85%)]\tLoss: 0.986284 Distill Loss: 1.402330, Student Loss: 0.015510\n",
            "Student Epoch: 2 [57600/60000 (96%)]\tLoss: 0.918564 Distill Loss: 1.303383, Student Loss: 0.020653\n",
            "Student Epoch 2 Average Loss: 1.485089, Average Distillation Loss: 2.039994, Average Student Loss: 0.190309\n",
            "Student Epoch: 3 [0/60000 (0%)]\tLoss: 1.312593 Distill Loss: 1.725770, Student Loss: 0.348514\n",
            "Student Epoch: 3 [6400/60000 (11%)]\tLoss: 1.132222 Distill Loss: 1.571652, Student Loss: 0.106884\n",
            "Student Epoch: 3 [12800/60000 (21%)]\tLoss: 1.462914 Distill Loss: 1.892441, Student Loss: 0.460684\n",
            "Student Epoch: 3 [19200/60000 (32%)]\tLoss: 1.292546 Distill Loss: 1.839152, Student Loss: 0.017132\n",
            "Student Epoch: 3 [25600/60000 (43%)]\tLoss: 0.936455 Distill Loss: 1.301665, Student Loss: 0.084297\n",
            "Student Epoch: 3 [32000/60000 (53%)]\tLoss: 1.097700 Distill Loss: 1.520822, Student Loss: 0.110416\n",
            "Student Epoch: 3 [38400/60000 (64%)]\tLoss: 1.706748 Distill Loss: 2.289942, Student Loss: 0.345964\n",
            "Student Epoch: 3 [44800/60000 (75%)]\tLoss: 1.173597 Distill Loss: 1.636223, Student Loss: 0.094137\n",
            "Student Epoch: 3 [51200/60000 (85%)]\tLoss: 1.112001 Distill Loss: 1.567856, Student Loss: 0.048338\n",
            "Student Epoch: 3 [57600/60000 (96%)]\tLoss: 0.841800 Distill Loss: 1.201560, Student Loss: 0.002361\n",
            "Student Epoch 3 Average Loss: 1.234251, Average Distillation Loss: 1.703532, Average Student Loss: 0.139261\n",
            "Student training time (with distillation): 214.22 seconds\n",
            "\n",
            "Test set: Accuracy: 9698/10000 (96.98%)\n",
            "\n",
            "Teacher Epoch: 1 [0/60000 (0%)]\tLoss: 2.314171\n",
            "Teacher Epoch: 1 [6400/60000 (11%)]\tLoss: 0.472822\n",
            "Teacher Epoch: 1 [12800/60000 (21%)]\tLoss: 0.286103\n",
            "Teacher Epoch: 1 [19200/60000 (32%)]\tLoss: 0.320913\n",
            "Teacher Epoch: 1 [25600/60000 (43%)]\tLoss: 0.208941\n",
            "Teacher Epoch: 1 [32000/60000 (53%)]\tLoss: 0.169407\n",
            "Teacher Epoch: 1 [38400/60000 (64%)]\tLoss: 0.181138\n",
            "Teacher Epoch: 1 [44800/60000 (75%)]\tLoss: 0.096001\n",
            "Teacher Epoch: 1 [51200/60000 (85%)]\tLoss: 0.129176\n",
            "Teacher Epoch: 1 [57600/60000 (96%)]\tLoss: 0.189483\n",
            "Teacher Epoch 1 Average Loss: 0.290976\n",
            "Teacher Epoch: 2 [0/60000 (0%)]\tLoss: 0.207819\n",
            "Teacher Epoch: 2 [6400/60000 (11%)]\tLoss: 0.061534\n",
            "Teacher Epoch: 2 [12800/60000 (21%)]\tLoss: 0.052211\n",
            "Teacher Epoch: 2 [19200/60000 (32%)]\tLoss: 0.125402\n",
            "Teacher Epoch: 2 [25600/60000 (43%)]\tLoss: 0.074200\n",
            "Teacher Epoch: 2 [32000/60000 (53%)]\tLoss: 0.199155\n",
            "Teacher Epoch: 2 [38400/60000 (64%)]\tLoss: 0.114536\n",
            "Teacher Epoch: 2 [44800/60000 (75%)]\tLoss: 0.130775\n",
            "Teacher Epoch: 2 [51200/60000 (85%)]\tLoss: 0.135325\n",
            "Teacher Epoch: 2 [57600/60000 (96%)]\tLoss: 0.286889\n",
            "Teacher Epoch 2 Average Loss: 0.142389\n",
            "Teacher Epoch: 3 [0/60000 (0%)]\tLoss: 0.100351\n",
            "Teacher Epoch: 3 [6400/60000 (11%)]\tLoss: 0.036840\n",
            "Teacher Epoch: 3 [12800/60000 (21%)]\tLoss: 0.084051\n",
            "Teacher Epoch: 3 [19200/60000 (32%)]\tLoss: 0.130607\n",
            "Teacher Epoch: 3 [25600/60000 (43%)]\tLoss: 0.096768\n",
            "Teacher Epoch: 3 [32000/60000 (53%)]\tLoss: 0.010047\n",
            "Teacher Epoch: 3 [38400/60000 (64%)]\tLoss: 0.091272\n",
            "Teacher Epoch: 3 [44800/60000 (75%)]\tLoss: 0.219472\n",
            "Teacher Epoch: 3 [51200/60000 (85%)]\tLoss: 0.094478\n",
            "Teacher Epoch: 3 [57600/60000 (96%)]\tLoss: 0.064537\n",
            "Teacher Epoch 3 Average Loss: 0.112699\n",
            "Student training time (no distillation): 58.65 seconds\n",
            "\n",
            "Test set: Accuracy: 9751/10000 (97.51%)\n",
            "\n",
            "--------------------\n",
            "Results:\n",
            "Teacher Accuracy: 99.07%\n",
            "Distilled Student Accuracy: 96.98%\n",
            "Baseline Student Accuracy (no distillation): 97.51%\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jxZyahVYITa9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}